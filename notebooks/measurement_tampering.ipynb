{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background / Intro\n",
    "\n",
    "This notebook trains a measurement predictor on trusted and untrusted data in the diamonds data-set\n",
    "trains a \"ground-truth\" probe using the ground truth diamond labels, and evaluates \n",
    "a \"confidence-based detector\" -using confidence in prediction of aggregated measurments\n",
    "as a detector for anomolous examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from cupbearer import data, detectors, models, scripts, tasks, utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "import transformers\n",
    "import torch\n",
    "import submitit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_HPARAMS = {\n",
    "    \"model\": \"pythia-14m\",\n",
    "    \"batch_size_on_device\": 4,\n",
    "    \"num_epochs\": 1,\n",
    "    \"dataset_len\": 64,\n",
    "    \"slurm_params\": {}\n",
    "}\n",
    "REAL_HPARAMS = {\n",
    "    \"model\": \"code-gen\",\n",
    "    \"batch_size_on_device\": 4, \n",
    "    \"num_epochs\": 5, \n",
    "    \"dataset_len\": None,\n",
    "    \"slurm_params\": {\n",
    "        \"slurm_mem_gb\": 80, \n",
    "        \"gres\": \"gpu:A100-SXM4-80GB:1\",\n",
    "        \"nodes\": 1, \n",
    "        \"timeout_min\": 60 * 10,\n",
    "        \"job_name\": \"bash\",\n",
    "        \"qos\": \"high\"\n",
    "    }\n",
    "}\n",
    "\n",
    "HPARAMS = LOCAL_HPARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer, tokenizer, emb_dim, max_len = models.transformers_hf.load_transformer(\n",
    "    HPARAMS[\"model\"]\n",
    ")\n",
    "model = models.TamperingPredictionTransformer(\n",
    "        model=transformer,\n",
    "        embed_dim=emb_dim\n",
    "    )\n",
    "tokenizer = model.set_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.TamperingDataset(\"diamonds\", tokenizer=tokenizer, max_length=max_len, \n",
    "                                   train=True, dataset_len=HPARAMS[\"dataset_len\"])\n",
    "val_data = data.TamperingDataset(\"diamonds\", tokenizer=tokenizer, max_length=max_len, \n",
    "                                 train=False, dataset_len=HPARAMS[\"dataset_len\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Experiment Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = os.path.abspath(utils.log_path(\"logs/tampering/predictor\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Measurement Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import DeviceStatsMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_dir = os.path.join(exp_dir, \"train_pred\")\n",
    "os.makedirs(train_pred_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "weight_decay = 2e-2\n",
    "num_warmup_steps = 64\n",
    "batch_size_base = 32\n",
    "precision=\"16-mixed\"\n",
    "\n",
    "batch_size_on_device = HPARAMS[\"batch_size_on_device\"]\n",
    "accumulate_grad_batches = batch_size_base // batch_size_on_device\n",
    "num_epochs = HPARAMS[\"num_epochs\"]\n",
    "loss_weights = [0.7, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size_on_device, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size_on_device, shuffle=False)\n",
    "total_steps = num_epochs * len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = lambda logits, labels: \\\n",
    "    torch.nn.functional.binary_cross_entropy_with_logits(logits[:, :3], labels[:, :3]) * loss_weights[0] + \\\n",
    "    torch.nn.functional.binary_cross_entropy_with_logits(logits[:, 3], labels[:, 3]) * loss_weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = submitit.AutoExecutor(folder=train_pred_dir)\n",
    "executor.update_parameters(**HPARAMS[\"slurm_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = executor.submit(scripts.train_classifier,\n",
    "    path=exp_dir,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    task=\"multilabel\",\n",
    "    num_labels=4,\n",
    "    val_loaders=val_loader,\n",
    "    optim_builder=torch.optim.AdamW,\n",
    "    optim_conf={\"lr\": lr, \"weight_decay\": weight_decay},\n",
    "    lr_scheduler_conf={\n",
    "        \"num_warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": total_steps\n",
    "    },\n",
    "    lr_scheduler_builder=scripts.lr_scheduler.CosineWarmupScheduler,\n",
    "    max_epochs=num_epochs,\n",
    "    wandb=False,\n",
    "    callbacks=[DeviceStatsMonitor()],\n",
    "    precision=precision,\n",
    "    accumulate_grad_batches=accumulate_grad_batches,\n",
    "    loss_func=loss_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Measurement Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pred_dir = os.path.join(exp_dir, \"eval_job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification.auroc import BinaryAUROC\n",
    "from cupbearer.scripts.metrics import DictOutWrapper, WeightedBinaryAUROCBootStrapper, MultioutDictWrapper\n",
    "\n",
    "multi_label_auroc_dict = DictOutWrapper(\n",
    "    base_metric=WeightedBinaryAUROCBootStrapper(base_metric=BinaryAUROC(), out_dict=True),\n",
    "    keys=[\"mean\", \"std\"]\n",
    ")\n",
    "\n",
    "multi_label_auroc = MultioutDictWrapper(\n",
    "    base_metric=multi_label_auroc_dict, \n",
    "    num_outputs=model.n_probes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use Subset dataset? or construct new measuremnt tamp\n",
    "val_data_dirty = Subset(val_data, [i for i, el in enumerate(val_data)\n",
    "                                    if not el[\"info\"][\"clean\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = submitit.AutoExecutor(folder=eval_pred_dir)\n",
    "executor.update_parameters(**HPARAMS[\"slurm_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pred_job = executor.submit(scripts.eval_classifier,\n",
    "    data=val_data_dirty,\n",
    "    model=model, \n",
    "    path=exp_dir,\n",
    "    batch_size=HPARAMS[\"batch_size_on_device\"],\n",
    "    test_metrics={\"auroc\": multi_label_auroc}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_task = tasks.measurement_tampering(\n",
    "    model=model, train_data=train_data, test_data=val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Ground-Truth Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cupbearer.models.transformers_hf import TransformerBaseHF, TokenDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalTokenProbe(torch.nn.Module):\n",
    "    def __init__(self, model: TransformerBaseHF):\n",
    "        super().__init__()\n",
    "        self.model = model \n",
    "        self.probe = torch.nn.Linear(model.embed_dim, 1)\n",
    "    \n",
    "    def forward(self, x: TokenDict, **emb_kwargs):\n",
    "        embs = self.model.get_embeddings(\n",
    "            {k: x[k] for k in TokenDict.__required_keys__}, **emb_kwargs)\n",
    "        last_emb = embs[:, -1, :]\n",
    "        out = self.probe(last_emb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = copy.deepcopy(model)\n",
    "models.load(ft_model, exp_dir, map_location=\"cpu\")\n",
    "gt_probe_model = FinalTokenProbe(model=ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_func = lambda y, info: info[\"correct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts.train_detector(\n",
    "    task=\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use finetuning detector\n",
    "# just add probe to model\n",
    "# I guess create \"probe detector\"\n",
    "# and then use y_func to train on info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Ground-Truth Probe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupbearer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
