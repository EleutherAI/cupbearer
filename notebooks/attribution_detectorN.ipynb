{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, reduce\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from cupbearer import detectors, tasks, utils, scripts\n",
    "from torch import Tensor, nn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Nora Belrose\n",
    "@contextmanager\n",
    "def atp(model: nn.Module, noise_acts: dict[str, Tensor], *, head_dim: int = 0):\n",
    "    \"\"\"Perform attribution patching on `model` with `noise_acts`.\n",
    "\n",
    "    This function adds forward and backward hooks to submodules of `model`\n",
    "    so that when you run a forward pass, the relevant activations are cached\n",
    "    in a dictionary, and when you run a backward pass, the gradients w.r.t.\n",
    "    activations are used to compute approximate causal effects.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to patch.\n",
    "        noise_acts (dict[str, Tensor]): A dictionary mapping (suffixes of) module\n",
    "            paths to noise activations.\n",
    "        head_dim (int): The size of each attention head, if applicable. When nonzero,\n",
    "            the effects are returned with a head dimension.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    noise = {\n",
    "        \"model.encoder.layer.0.attention.self\": torch.zeros(1, 12, 64, 64),\n",
    "    }\n",
    "    with atp(model, noise) as effects:\n",
    "        probs = model(input_ids).logits.softmax(-1)\n",
    "        probs[0].backward()\n",
    "\n",
    "    # Use the effects\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Keep track of all the hooks we're adding to the model\n",
    "    handles: list[nn.modules.module.RemovableHandle] = []\n",
    "\n",
    "    # Keep track of activations from the forward pass\n",
    "    mod_to_clean: dict[nn.Module, Tensor] = {}\n",
    "    mod_to_noise: dict[nn.Module, Tensor] = {}\n",
    "\n",
    "    # Dictionary of effects\n",
    "    effects: dict[str, Tensor] = {}\n",
    "    mod_to_name: dict[nn.Module, str] = {}\n",
    "\n",
    "    # Backward hook\n",
    "    def bwd_hook(module: nn.Module, _, grad_output: tuple[Tensor, ...] | Tensor):\n",
    "        # Unpack the gradient output if it's a tuple\n",
    "        if isinstance(grad_output, tuple):\n",
    "            grad_output, *_ = grad_output\n",
    "\n",
    "        # Use pop() to ensure we don't use the same activation multiple times\n",
    "        # and to save memory\n",
    "        clean = mod_to_clean.pop(module)\n",
    "        direction = mod_to_noise[module] - clean\n",
    "\n",
    "        # Group heads together if applicable\n",
    "        if head_dim > 0:\n",
    "            direction = direction.unflatten(-1, (-1, head_dim))\n",
    "            grad_output = grad_output.unflatten(-1, (-1, head_dim))\n",
    "\n",
    "        # Batched dot product\n",
    "        effect = torch.linalg.vecdot(direction, grad_output.type_as(direction))\n",
    "\n",
    "        # Save the effect\n",
    "        name = mod_to_name[module]\n",
    "        effects[name] = effect\n",
    "\n",
    "    # Forward hook\n",
    "    def fwd_hook(module: nn.Module, _, output: tuple[Tensor, ...] | Tensor):\n",
    "        # Unpack the output if it's a tuple\n",
    "        if isinstance(output, tuple):\n",
    "            output, *_ = output\n",
    "\n",
    "        mod_to_clean[module] = output.detach()\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        # Hooks need to be able to look up the name of a module\n",
    "        mod_to_name[module] = name\n",
    "\n",
    "        # Check if the module is in the paths\n",
    "        for path, noise in noise_acts.items():\n",
    "            if not name.endswith(path):\n",
    "                continue\n",
    "\n",
    "            # Add a hook to the module\n",
    "            handles.append(module.register_full_backward_hook(bwd_hook))\n",
    "            handles.append(module.register_forward_hook(fwd_hook))\n",
    "\n",
    "            # Save the noise activation\n",
    "            mod_to_noise[module] = noise\n",
    "\n",
    "    try:\n",
    "        yield effects\n",
    "    finally:\n",
    "        # Remove all hooks\n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "\n",
    "        # Clear grads on the model just to be safe\n",
    "        model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sciq = load_dataset(\"ejenner/quirky_sciq_raw\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '066e7164',\n",
       " 'template_args': {'answer': 'amino acids',\n",
       "  'character': 'Alice',\n",
       "  'question': 'Both fats and oils are made up of long chains of carbon atoms that are bonded together. what are these chains called?',\n",
       "  'support': 'Lipids consist only or mainly of carbon, hydrogen, and oxygen. Both fats and oils are made up of long chains of carbon atoms that are bonded together. These chains are called fatty acids. Fatty acids may be saturated or unsaturated. In the Figure below you can see structural formulas for two small fatty acids, one saturated and one unsaturated.'},\n",
       " 'character': 'Alice',\n",
       " 'label': False,\n",
       " 'alice_label': False,\n",
       " 'bob_label': False,\n",
       " 'difficulty': 0,\n",
       " 'difficulty_quantile': 1.0237510237510238e-05}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sciq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/david/miniconda3/envs/cupbearer/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:31<00:00, 15.55s/it]\n",
      "\u001b[32m2024-05-16 02:19:10.933\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.tasks.quirky_lm\u001b[0m:\u001b[36mquirky_lm\u001b[0m:\u001b[36m106\u001b[0m - \u001b[34m\u001b[1mAlice trusted: 11186 samples\u001b[0m\n",
      "\u001b[32m2024-05-16 02:19:10.935\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.tasks.quirky_lm\u001b[0m:\u001b[36mquirky_lm\u001b[0m:\u001b[36m107\u001b[0m - \u001b[34m\u001b[1mAlice test: 1000 samples\u001b[0m\n",
      "\u001b[32m2024-05-16 02:19:10.935\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.tasks.quirky_lm\u001b[0m:\u001b[36mquirky_lm\u001b[0m:\u001b[36m108\u001b[0m - \u001b[34m\u001b[1mBob test: 1000 samples\u001b[0m\n",
      "\u001b[32m2024-05-16 02:19:10.936\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.tasks.quirky_lm\u001b[0m:\u001b[36mquirky_lm\u001b[0m:\u001b[36m110\u001b[0m - \u001b[34m\u001b[1mAlice untrusted: 11186 samples\u001b[0m\n",
      "\u001b[32m2024-05-16 02:19:10.936\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.tasks.quirky_lm\u001b[0m:\u001b[36mquirky_lm\u001b[0m:\u001b[36m111\u001b[0m - \u001b[34m\u001b[1mBob untrusted: 22372 samples\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# task = tasks.tiny_natural_mechanisms(\"hex\", device=\"cuda\")\n",
    "task = tasks.quirky_lm(include_untrusted=True, mixture=True, standardize_template=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_token = task.model.tokenizer.encode(' No', add_special_tokens=False)[-1]\n",
    "yes_token = task.model.tokenizer.encode(' Yes', add_special_tokens=False)[-1]\n",
    "effect_tokens = torch.tensor([no_token, yes_token], dtype=torch.long, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hf_model\n",
      "hf_model.base_model\n",
      "hf_model.base_model.model\n",
      "hf_model.base_model.model.model\n",
      "hf_model.base_model.model.model.embed_tokens\n",
      "hf_model.base_model.model.model.layers\n",
      "hf_model.base_model.model.model.layers.0\n",
      "hf_model.base_model.model.model.layers.0.self_attn\n",
      "hf_model.base_model.model.model.layers.0.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.0.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.0.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.0.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.0.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.0.mlp\n",
      "hf_model.base_model.model.model.layers.0.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.0.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.0.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.0.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.0.input_layernorm\n",
      "hf_model.base_model.model.model.layers.0.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.1\n",
      "hf_model.base_model.model.model.layers.1.self_attn\n",
      "hf_model.base_model.model.model.layers.1.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.1.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.1.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.1.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.1.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.1.mlp\n",
      "hf_model.base_model.model.model.layers.1.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.1.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.1.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.1.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.1.input_layernorm\n",
      "hf_model.base_model.model.model.layers.1.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.2\n",
      "hf_model.base_model.model.model.layers.2.self_attn\n",
      "hf_model.base_model.model.model.layers.2.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.2.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.2.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.2.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.2.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.2.mlp\n",
      "hf_model.base_model.model.model.layers.2.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.2.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.2.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.2.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.2.input_layernorm\n",
      "hf_model.base_model.model.model.layers.2.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.3\n",
      "hf_model.base_model.model.model.layers.3.self_attn\n",
      "hf_model.base_model.model.model.layers.3.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.3.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.3.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.3.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.3.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.3.mlp\n",
      "hf_model.base_model.model.model.layers.3.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.3.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.3.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.3.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.3.input_layernorm\n",
      "hf_model.base_model.model.model.layers.3.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.4\n",
      "hf_model.base_model.model.model.layers.4.self_attn\n",
      "hf_model.base_model.model.model.layers.4.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.4.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.4.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.4.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.4.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.4.mlp\n",
      "hf_model.base_model.model.model.layers.4.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.4.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.4.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.4.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.4.input_layernorm\n",
      "hf_model.base_model.model.model.layers.4.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.5\n",
      "hf_model.base_model.model.model.layers.5.self_attn\n",
      "hf_model.base_model.model.model.layers.5.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.5.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.5.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.5.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.5.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.5.mlp\n",
      "hf_model.base_model.model.model.layers.5.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.5.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.5.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.5.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.5.input_layernorm\n",
      "hf_model.base_model.model.model.layers.5.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.6\n",
      "hf_model.base_model.model.model.layers.6.self_attn\n",
      "hf_model.base_model.model.model.layers.6.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.6.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.6.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.6.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.6.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.6.mlp\n",
      "hf_model.base_model.model.model.layers.6.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.6.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.6.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.6.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.6.input_layernorm\n",
      "hf_model.base_model.model.model.layers.6.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.7\n",
      "hf_model.base_model.model.model.layers.7.self_attn\n",
      "hf_model.base_model.model.model.layers.7.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.7.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.7.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.7.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.7.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.7.mlp\n",
      "hf_model.base_model.model.model.layers.7.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.7.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.7.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.7.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.7.input_layernorm\n",
      "hf_model.base_model.model.model.layers.7.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.8\n",
      "hf_model.base_model.model.model.layers.8.self_attn\n",
      "hf_model.base_model.model.model.layers.8.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.8.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.8.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.8.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.8.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.8.mlp\n",
      "hf_model.base_model.model.model.layers.8.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.8.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.8.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.8.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.8.input_layernorm\n",
      "hf_model.base_model.model.model.layers.8.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.9\n",
      "hf_model.base_model.model.model.layers.9.self_attn\n",
      "hf_model.base_model.model.model.layers.9.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.9.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.9.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.9.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.9.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.9.mlp\n",
      "hf_model.base_model.model.model.layers.9.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.9.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.9.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.9.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.9.input_layernorm\n",
      "hf_model.base_model.model.model.layers.9.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.10\n",
      "hf_model.base_model.model.model.layers.10.self_attn\n",
      "hf_model.base_model.model.model.layers.10.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.10.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.10.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.10.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.10.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.10.mlp\n",
      "hf_model.base_model.model.model.layers.10.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.10.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.10.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.10.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.10.input_layernorm\n",
      "hf_model.base_model.model.model.layers.10.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.11\n",
      "hf_model.base_model.model.model.layers.11.self_attn\n",
      "hf_model.base_model.model.model.layers.11.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.11.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.11.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.11.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.11.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.11.mlp\n",
      "hf_model.base_model.model.model.layers.11.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.11.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.11.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.11.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.11.input_layernorm\n",
      "hf_model.base_model.model.model.layers.11.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.12\n",
      "hf_model.base_model.model.model.layers.12.self_attn\n",
      "hf_model.base_model.model.model.layers.12.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.12.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.12.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.12.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.12.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.12.mlp\n",
      "hf_model.base_model.model.model.layers.12.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.12.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.12.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.12.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.12.input_layernorm\n",
      "hf_model.base_model.model.model.layers.12.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.13\n",
      "hf_model.base_model.model.model.layers.13.self_attn\n",
      "hf_model.base_model.model.model.layers.13.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.13.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.13.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.13.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.13.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.13.mlp\n",
      "hf_model.base_model.model.model.layers.13.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.13.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.13.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.13.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.13.input_layernorm\n",
      "hf_model.base_model.model.model.layers.13.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.14\n",
      "hf_model.base_model.model.model.layers.14.self_attn\n",
      "hf_model.base_model.model.model.layers.14.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.14.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.14.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.14.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.14.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.14.mlp\n",
      "hf_model.base_model.model.model.layers.14.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.14.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.14.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.14.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.14.input_layernorm\n",
      "hf_model.base_model.model.model.layers.14.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.15\n",
      "hf_model.base_model.model.model.layers.15.self_attn\n",
      "hf_model.base_model.model.model.layers.15.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.15.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.15.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.15.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.15.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.15.mlp\n",
      "hf_model.base_model.model.model.layers.15.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.15.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.15.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.15.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.15.input_layernorm\n",
      "hf_model.base_model.model.model.layers.15.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.16\n",
      "hf_model.base_model.model.model.layers.16.self_attn\n",
      "hf_model.base_model.model.model.layers.16.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.16.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.16.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.16.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.16.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.16.mlp\n",
      "hf_model.base_model.model.model.layers.16.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.16.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.16.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.16.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.16.input_layernorm\n",
      "hf_model.base_model.model.model.layers.16.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.17\n",
      "hf_model.base_model.model.model.layers.17.self_attn\n",
      "hf_model.base_model.model.model.layers.17.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.17.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.17.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.17.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.17.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.17.mlp\n",
      "hf_model.base_model.model.model.layers.17.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.17.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.17.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.17.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.17.input_layernorm\n",
      "hf_model.base_model.model.model.layers.17.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.18\n",
      "hf_model.base_model.model.model.layers.18.self_attn\n",
      "hf_model.base_model.model.model.layers.18.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.18.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.18.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.18.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.18.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.18.mlp\n",
      "hf_model.base_model.model.model.layers.18.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.18.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.18.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.18.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.18.input_layernorm\n",
      "hf_model.base_model.model.model.layers.18.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.19\n",
      "hf_model.base_model.model.model.layers.19.self_attn\n",
      "hf_model.base_model.model.model.layers.19.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.19.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.19.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.19.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.19.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.19.mlp\n",
      "hf_model.base_model.model.model.layers.19.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.19.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.19.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.19.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.19.input_layernorm\n",
      "hf_model.base_model.model.model.layers.19.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.20\n",
      "hf_model.base_model.model.model.layers.20.self_attn\n",
      "hf_model.base_model.model.model.layers.20.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.20.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.20.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.20.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.20.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.20.mlp\n",
      "hf_model.base_model.model.model.layers.20.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.20.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.20.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.20.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.20.input_layernorm\n",
      "hf_model.base_model.model.model.layers.20.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.21\n",
      "hf_model.base_model.model.model.layers.21.self_attn\n",
      "hf_model.base_model.model.model.layers.21.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.21.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.21.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.21.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.21.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.21.mlp\n",
      "hf_model.base_model.model.model.layers.21.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.21.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.21.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.21.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.21.input_layernorm\n",
      "hf_model.base_model.model.model.layers.21.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.22\n",
      "hf_model.base_model.model.model.layers.22.self_attn\n",
      "hf_model.base_model.model.model.layers.22.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.22.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.22.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.22.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.22.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.22.mlp\n",
      "hf_model.base_model.model.model.layers.22.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.22.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.22.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.22.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.22.input_layernorm\n",
      "hf_model.base_model.model.model.layers.22.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.23\n",
      "hf_model.base_model.model.model.layers.23.self_attn\n",
      "hf_model.base_model.model.model.layers.23.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.23.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.23.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.23.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.23.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.23.mlp\n",
      "hf_model.base_model.model.model.layers.23.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.23.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.23.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.23.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.23.input_layernorm\n",
      "hf_model.base_model.model.model.layers.23.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.24\n",
      "hf_model.base_model.model.model.layers.24.self_attn\n",
      "hf_model.base_model.model.model.layers.24.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.24.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.24.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.24.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.24.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.24.mlp\n",
      "hf_model.base_model.model.model.layers.24.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.24.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.24.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.24.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.24.input_layernorm\n",
      "hf_model.base_model.model.model.layers.24.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.25\n",
      "hf_model.base_model.model.model.layers.25.self_attn\n",
      "hf_model.base_model.model.model.layers.25.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.25.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.25.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.25.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.25.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.25.mlp\n",
      "hf_model.base_model.model.model.layers.25.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.25.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.25.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.25.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.25.input_layernorm\n",
      "hf_model.base_model.model.model.layers.25.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.26\n",
      "hf_model.base_model.model.model.layers.26.self_attn\n",
      "hf_model.base_model.model.model.layers.26.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.26.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.26.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.26.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.26.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.26.mlp\n",
      "hf_model.base_model.model.model.layers.26.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.26.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.26.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.26.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.26.input_layernorm\n",
      "hf_model.base_model.model.model.layers.26.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.27\n",
      "hf_model.base_model.model.model.layers.27.self_attn\n",
      "hf_model.base_model.model.model.layers.27.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.27.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.27.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.27.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.27.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.27.mlp\n",
      "hf_model.base_model.model.model.layers.27.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.27.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.27.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.27.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.27.input_layernorm\n",
      "hf_model.base_model.model.model.layers.27.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.28\n",
      "hf_model.base_model.model.model.layers.28.self_attn\n",
      "hf_model.base_model.model.model.layers.28.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.28.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.28.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.28.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.28.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.28.mlp\n",
      "hf_model.base_model.model.model.layers.28.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.28.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.28.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.28.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.28.input_layernorm\n",
      "hf_model.base_model.model.model.layers.28.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.29\n",
      "hf_model.base_model.model.model.layers.29.self_attn\n",
      "hf_model.base_model.model.model.layers.29.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.29.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.29.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.29.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.29.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.29.mlp\n",
      "hf_model.base_model.model.model.layers.29.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.29.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.29.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.29.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.29.input_layernorm\n",
      "hf_model.base_model.model.model.layers.29.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.30\n",
      "hf_model.base_model.model.model.layers.30.self_attn\n",
      "hf_model.base_model.model.model.layers.30.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.30.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.30.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.30.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.30.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.30.mlp\n",
      "hf_model.base_model.model.model.layers.30.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.30.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.30.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.30.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.30.input_layernorm\n",
      "hf_model.base_model.model.model.layers.30.post_attention_layernorm\n",
      "hf_model.base_model.model.model.layers.31\n",
      "hf_model.base_model.model.model.layers.31.self_attn\n",
      "hf_model.base_model.model.model.layers.31.self_attn.q_proj\n",
      "hf_model.base_model.model.model.layers.31.self_attn.k_proj\n",
      "hf_model.base_model.model.model.layers.31.self_attn.v_proj\n",
      "hf_model.base_model.model.model.layers.31.self_attn.o_proj\n",
      "hf_model.base_model.model.model.layers.31.self_attn.rotary_emb\n",
      "hf_model.base_model.model.model.layers.31.mlp\n",
      "hf_model.base_model.model.model.layers.31.mlp.gate_proj\n",
      "hf_model.base_model.model.model.layers.31.mlp.up_proj\n",
      "hf_model.base_model.model.model.layers.31.mlp.down_proj\n",
      "hf_model.base_model.model.model.layers.31.mlp.act_fn\n",
      "hf_model.base_model.model.model.layers.31.input_layernorm\n",
      "hf_model.base_model.model.model.layers.31.post_attention_layernorm\n",
      "hf_model.base_model.model.model.norm\n",
      "hf_model.base_model.model.lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, _ in task.model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"_name_or_path\": \"mistralai/Mistral-7B-v0.1\",\n",
       "  \"architectures\": [\n",
       "    \"MistralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.40.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.model.hf_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_outlier_factor(X, k):\n",
    "    # Calculate pairwise squared Euclidean distances\n",
    "    dist = torch.cdist(X, X).fill_diagonal_(torch.inf)\n",
    "    distances, indices = dist.topk(k, largest=False)\n",
    "\n",
    "    # Calculate reachability distances\n",
    "    k_dists = distances[:, -1, None].expand_as(distances)\n",
    "    lrd = torch.max(distances, k_dists).mean(dim=1).reciprocal()\n",
    "\n",
    "    lrd_ratios = lrd[indices] / lrd[:, None]\n",
    "    return lrd_ratios.sum(dim=1) / k\n",
    "\n",
    "def isolation_forest(X):\n",
    "    res = IsolationForest().fit_predict(X)\n",
    "\n",
    "class AttributionDetector(detectors.AnomalyDetector):\n",
    "    def __init__(self, shapes: dict[str, tuple[int, ...]], output_func):\n",
    "        super().__init__()\n",
    "        self.shapes = shapes\n",
    "        self.output_func = output_func\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def train(\n",
    "        self,\n",
    "        trusted_data: torch.utils.data.Dataset,\n",
    "        untrusted_data: torch.utils.data.Dataset | None,\n",
    "        save_path: Path | str | None,\n",
    "        batch_size: int = 1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        assert trusted_data is not None\n",
    "\n",
    "        dtype = task.model.hf_model.dtype\n",
    "        device = task.model.hf_model.device\n",
    "\n",
    "        # Why shape[-2]? We are going to sum over the last dimension during attribution\n",
    "        # patching. We'll then use the second-to-last dimension as our main dimension\n",
    "        # to fit Gaussians to (all earlier dimensions will be summed out first).\n",
    "        # This is kind of arbitrary and we're putting the onus on the user to make\n",
    "        # sure this makes sense.\n",
    "        self._means = {\n",
    "            name: torch.zeros(32, device=device)\n",
    "            for name, shape in self.shapes.items()\n",
    "        }\n",
    "        self._Cs = {\n",
    "            name: torch.zeros(32, 32, device=device)\n",
    "            for name, shape in self.shapes.items()\n",
    "        }\n",
    "        self._n = 0\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(trusted_data, batch_size=batch_size)\n",
    "        for batch in tqdm(dataloader):\n",
    "            inputs = utils.inputs_from_batch(batch)\n",
    "            noise = {\n",
    "                name: torch.zeros((batch_size, 1, *shape), device=device, dtype=dtype)\n",
    "                for name, shape in self.shapes.items()\n",
    "            }\n",
    "            with atp(task.model, noise, head_dim=128) as effects:\n",
    "                out = self.model(inputs).logits\n",
    "                out = self.output_func(out)\n",
    "                # assert out.shape == (batch_size,), out.shape\n",
    "                out.backward()\n",
    "\n",
    "            self._n += batch_size\n",
    "\n",
    "            for name, effect in effects.items():\n",
    "                effect = reduce(\n",
    "                    effect, \"batch ... h -> batch h\", \"sum\", batch=batch_size\n",
    "                )\n",
    "                self._means[name], self._Cs[name], _ = (\n",
    "                    detectors.statistical.helpers.update_covariance(\n",
    "                        self._means[name], self._Cs[name], self._n, effect\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.means = self._means\n",
    "        self.covariances = {k: C / (self._n - 1) for k, C in self._Cs.items()}\n",
    "        if any(torch.count_nonzero(C) == 0 for C in self.covariances.values()):\n",
    "            raise RuntimeError(\"All zero covariance matrix detected.\")\n",
    "\n",
    "        self.inv_covariances = {\n",
    "            k: detectors.statistical.mahalanobis_detector._pinv(C, rcond=1e-5)\n",
    "            for k, C in self.covariances.items()\n",
    "        }\n",
    "\n",
    "    def layerwise_scores(self, batch):\n",
    "        inputs = utils.inputs_from_batch(batch)\n",
    "        batch_size = len(inputs)\n",
    "        noise = {\n",
    "            name: torch.zeros((batch_size, *shape), device=\"cuda\")\n",
    "            for name, shape in self.shapes.items()\n",
    "        }\n",
    "        # AnomalyDetector.eval() wraps everything in a no_grad block, need to undo that.\n",
    "        with torch.enable_grad():\n",
    "            with atp(task.model, noise, head_dim=128) as effects:\n",
    "                out = self.model(inputs).logits\n",
    "                out = self.output_func(out)\n",
    "                # assert out.shape == (batch_size,), out.shape\n",
    "                out.backward()\n",
    "                # self.sample_grad_func(inputs)\n",
    "\n",
    "        for name, effect in effects.items():\n",
    "            effects[name] = reduce(\n",
    "                effect, \"batch ... h -> batch h\", \"sum\", batch=batch_size\n",
    "            )\n",
    "\n",
    "        # distances = detectors.statistical.helpers.mahalanobis(\n",
    "        #     effects,\n",
    "        #     self.means,\n",
    "        #     self.inv_covariances,\n",
    "        # )\n",
    "        distances: dict[str, torch.Tensor] = {}\n",
    "\n",
    "        for name, effect in effects.items():\n",
    "            distances[name] = local_outlier_factor(effect, 20)\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def _get_trained_variables(self, saving: bool = False):\n",
    "        return {\n",
    "            \"means\": self.means,\n",
    "            \"inv_covariances\": self.inv_covariances,\n",
    "        }\n",
    "\n",
    "    def _set_trained_variables(self, variables):\n",
    "        self.means = variables[\"means\"]\n",
    "        self.inv_covariances = variables[\"inv_covariances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effect_prob_func(logits):\n",
    "    assert logits.ndim == 3\n",
    "    probs = logits.softmax(-1)\n",
    "\n",
    "    return probs[:, -1, effect_tokens].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cupbearer.data.huggingface.HuggingfaceDataset at 0x7f804c30e890>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.trusted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2797 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2797 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (236) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m emb \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhf_model\u001b[38;5;241m.\u001b[39mget_input_embeddings()\n\u001b[1;32m      8\u001b[0m emb\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mscripts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ssd-1/david/cupbearer/src/cupbearer/scripts/train_detector.py:18\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(task, detector, save_path, eval_batch_size, **train_kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(\n\u001b[1;32m     10\u001b[0m     task: Task,\n\u001b[1;32m     11\u001b[0m     detector: AnomalyDetector,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_kwargs,\n\u001b[1;32m     15\u001b[0m ):\n\u001b[1;32m     16\u001b[0m     detector\u001b[38;5;241m.\u001b[39mset_model(task\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrusted_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrusted_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43muntrusted_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntrusted_train_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_path:\n\u001b[1;32m     25\u001b[0m         save_path \u001b[38;5;241m=\u001b[39m Path(save_path)\n",
      "File \u001b[0;32m/mnt/ssd-1/david/miniconda3/envs/cupbearer/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 62\u001b[0m, in \u001b[0;36mAttributionDetector.train\u001b[0;34m(self, trusted_data, untrusted_data, save_path, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_func(out)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# assert out.shape == (batch_size,), out.shape\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, effect \u001b[38;5;129;01min\u001b[39;00m effects\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/mnt/ssd-1/david/miniconda3/envs/cupbearer/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ssd-1/david/miniconda3/envs/cupbearer/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ssd-1/david/miniconda3/envs/cupbearer/lib/python3.11/site-packages/torch/utils/hooks.py:221\u001b[0m, in \u001b[0;36mBackwardHook.setup_output_hook.<locals>.fn.<locals>.hook\u001b[0;34m(_, grad_output)\u001b[0m\n\u001b[1;32m    219\u001b[0m grad_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pack_with_none([], [], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_inputs)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_hooks:\n\u001b[0;32m--> 221\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43muser_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m res)):\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackward hook for Modules where no input requires \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient should always return None or None for all gradients.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 50\u001b[0m, in \u001b[0;36matp.<locals>.bwd_hook\u001b[0;34m(module, _, grad_output)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Use pop() to ensure we don't use the same activation multiple times\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# and to save memory\u001b[39;00m\n\u001b[1;32m     49\u001b[0m clean \u001b[38;5;241m=\u001b[39m mod_to_clean\u001b[38;5;241m.\u001b[39mpop(module)\n\u001b[0;32m---> 50\u001b[0m direction \u001b[38;5;241m=\u001b[39m \u001b[43mmod_to_noise\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Group heads together if applicable\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_dim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (236) at non-singleton dimension 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_1776941/4094933900.py\u001b[0m(50)\u001b[0;36mbwd_hook\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     48 \u001b[0;31m        \u001b[0;31m# and to save memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     49 \u001b[0;31m        \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod_to_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 50 \u001b[0;31m        \u001b[0mdirection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod_to_noise\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m        \u001b[0;31m# Group heads together if applicable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "torch.Size([4, 236, 4096])\n",
      "torch.Size([4, 236, 4096])\n",
      "torch.Size([4, 4096])\n",
      "*** NameError: name 'noise' is not defined\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "f\n"
     ]
    }
   ],
   "source": [
    "detector = AttributionDetector(\n",
    "    shapes={\"hf_model.base_model.model.model.layers.16.self_attn\": (4096,)}, output_func=effect_prob_func\n",
    ")\n",
    "\n",
    "# Set requres_grad = True on the input embeddings only, so that torch does a full\n",
    "# backward pass but doesn't store gradients for the rest of the model parameters.\n",
    "emb = task.model.hf_model.get_input_embeddings()\n",
    "emb.requires_grad_(True)\n",
    "\n",
    "scripts.train_detector(task, detector, batch_size = 4, save_path=None, eval_batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'blocks.0.attn.hook_z'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m px\u001b[38;5;241m.\u001b[39mimshow(detector\u001b[38;5;241m.\u001b[39mcovariances[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.0.attn.hook_z\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu())\n",
      "\u001b[0;31mKeyError\u001b[0m: 'blocks.0.attn.hook_z'"
     ]
    }
   ],
   "source": [
    "px.imshow(detector.covariances[\"blocks.0.attn.hook_z\"].cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupbearer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
